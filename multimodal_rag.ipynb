{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "95Mvc4VzmVCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a27cffb3-9fc2-428c-b0a5-c9b64df33b99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.12/dist-packages (1.26.6)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.12/dist-packages (0.3.31)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: langchain_groq in /usr/local/lib/python3.12/dist-packages (0.3.8)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.12/dist-packages (2.1.12)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.79)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.44)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: groq<1,>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from langchain_groq) (0.34.1)\n",
            "Requirement already satisfied: google-ai-generativelanguage<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (0.9.0)\n",
            "Requirement already satisfied: filetype<2,>=1.2 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.28.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.76.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain_groq) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain_groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.72.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain_groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.30.0->langchain_groq) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain pymupdf langchain_community transformers python-dotenv torch faiss-cpu langchain_groq langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "from langchain_core.documents import Document\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy as np\n",
        "from langchain.schema.messages import HumanMessage\n",
        "import os\n",
        "import base64\n",
        "import io\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "7ZhDhG_BPpM2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata\n",
        "load_dotenv()\n",
        "\n",
        "## set up the environment\n",
        "os.environ['GOOGLE_API_KEY']=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "### initialize the Clip Model for unified embeddings\n",
        "clip_model=CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor=CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep7zuYFrPpTc",
        "outputId": "edab6d8d-ef0b-4560-c159-8f449221a9c7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CLIPModel(\n",
              "  (text_model): CLIPTextTransformer(\n",
              "    (embeddings): CLIPTextEmbeddings(\n",
              "      (token_embedding): Embedding(49408, 512)\n",
              "      (position_embedding): Embedding(77, 512)\n",
              "    )\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (vision_model): CLIPVisionTransformer(\n",
              "    (embeddings): CLIPVisionEmbeddings(\n",
              "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "      (position_embedding): Embedding(50, 768)\n",
              "    )\n",
              "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (encoder): CLIPEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x CLIPEncoderLayer(\n",
              "          (self_attn): CLIPAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): CLIPMLP(\n",
              "            (activation_fn): QuickGELUActivation()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
              "  (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Embedding functions\n",
        "def embed_image(image_data):\n",
        "    \"\"\"Embed image using CLIP\"\"\"\n",
        "    if isinstance(image_data, str):  # If path\n",
        "        image = Image.open(image_data).convert(\"RGB\")\n",
        "    else:  # If PIL Image\n",
        "        image = image_data\n",
        "\n",
        "    inputs=clip_processor(images=image,return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        features = clip_model.get_image_features(**inputs)\n",
        "        # Normalize embeddings to unit vector\n",
        "        features = features / features.norm(dim=-1, keepdim=True)\n",
        "        return features.squeeze().numpy()\n",
        "\n",
        "def embed_text(text):\n",
        "    \"\"\"Embed text using CLIP.\"\"\"\n",
        "    inputs = clip_processor(\n",
        "        text=text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=77  # CLIP's max token length\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        features = clip_model.get_text_features(**inputs)\n",
        "        # Normalize embeddings\n",
        "        features = features / features.norm(dim=-1, keepdim=True)\n",
        "        return features.squeeze().numpy()"
      ],
      "metadata": {
        "id": "RtL9BnlHPpZE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Process PDF\n",
        "pdf_path=\"multimodal_sample.pdf\"\n",
        "doc=fitz.open(pdf_path)\n",
        "# Storage for all documents and embeddings\n",
        "all_docs = []\n",
        "all_embeddings = []\n",
        "image_data_store = {}  # Store actual image data for LLM\n",
        "\n",
        "# Text splitter\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)"
      ],
      "metadata": {
        "id": "KPEWYYlUSA0H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6Bf9NGxSAu0",
        "outputId": "1d3e38e2-a5d9-4a69-b52b-fea557253208"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document('multimodal_sample.pdf')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i,page in enumerate(doc):\n",
        "    ## process text\n",
        "    text=page.get_text()\n",
        "    if text.strip():\n",
        "        ##create temporary document for splitting\n",
        "        temp_doc = Document(page_content=text, metadata={\"page\": i, \"type\": \"text\"})\n",
        "        text_chunks = splitter.split_documents([temp_doc])\n",
        "\n",
        "        #Embed each chunk using CLIP\n",
        "        for chunk in text_chunks:\n",
        "            embedding = embed_text(chunk.page_content)\n",
        "            all_embeddings.append(embedding)\n",
        "            all_docs.append(chunk)\n",
        "\n",
        "\n",
        "\n",
        "    ## process images\n",
        "    ##Three Important Actions:\n",
        "\n",
        "    ##Convert PDF image to PIL format\n",
        "    ##Store as base64 for GPT-4V (which needs base64 images)\n",
        "    ##Create CLIP embedding for retrieval\n",
        "\n",
        "    for img_index, img in enumerate(page.get_images(full=True)):\n",
        "        try:\n",
        "            xref = img[0]\n",
        "            base_image = doc.extract_image(xref)\n",
        "            image_bytes = base_image[\"image\"]\n",
        "\n",
        "            # Convert to PIL Image\n",
        "            pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
        "\n",
        "            # Create unique identifier\n",
        "            image_id = f\"page_{i}_img_{img_index}\"\n",
        "\n",
        "            # Store image as base64 for later use with GPT-4V\n",
        "            buffered = io.BytesIO()\n",
        "            pil_image.save(buffered, format=\"PNG\")\n",
        "            img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
        "            image_data_store[image_id] = img_base64\n",
        "\n",
        "            # Embed image using CLIP\n",
        "            embedding = embed_image(pil_image)\n",
        "            all_embeddings.append(embedding)\n",
        "\n",
        "            # Create document for image\n",
        "            image_doc = Document(\n",
        "                page_content=f\"[Image: {image_id}]\",\n",
        "                metadata={\"page\": i, \"type\": \"image\", \"image_id\": image_id}\n",
        "            )\n",
        "            all_docs.append(image_doc)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {img_index} on page {i}: {e}\")\n",
        "            continue\n",
        "\n",
        "doc.close()"
      ],
      "metadata": {
        "id": "LaLzHGSpSArS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyBy7UCTSAof",
        "outputId": "7b7a969c-1325-435d-ca0a-d19b6cd018c3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'page': 0, 'type': 'text'}, page_content='Annual Revenue Overview\\nThis document summarizes the revenue trends across Q1, Q2, and Q3. As illustrated in the chart\\nbelow, revenue grew steadily with the highest growth recorded in Q3.\\nQ1 showed a moderate increase in revenue as new product lines were introduced. Q2 outperformed\\nQ1 due to marketing campaigns. Q3 had exponential growth due to global expansion.'),\n",
              " Document(metadata={'page': 0, 'type': 'image', 'image_id': 'page_0_img_0'}, page_content='[Image: page_0_img_0]')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create unified FAISS vector store with CLIP embeddings\n",
        "embeddings_array = np.array(all_embeddings)\n",
        "embeddings_array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlPHVf-eSAl6",
        "outputId": "95a22ace-5eaa-44ac-d9a0-7599c312c77b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.00267245,  0.01282999, -0.05183139, ..., -0.00385086,\n",
              "         0.02977718, -0.00010685],\n",
              "       [ 0.01732335, -0.01327693, -0.02427032, ...,  0.0899405 ,\n",
              "        -0.00272154,  0.03253041]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(all_docs,embeddings_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0Z6hCQHSAjJ",
        "outputId": "c1ea9363-ab27-43d2-a839-0b664531c196"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([Document(metadata={'page': 0, 'type': 'text'}, page_content='Annual Revenue Overview\\nThis document summarizes the revenue trends across Q1, Q2, and Q3. As illustrated in the chart\\nbelow, revenue grew steadily with the highest growth recorded in Q3.\\nQ1 showed a moderate increase in revenue as new product lines were introduced. Q2 outperformed\\nQ1 due to marketing campaigns. Q3 had exponential growth due to global expansion.'),\n",
              "  Document(metadata={'page': 0, 'type': 'image', 'image_id': 'page_0_img_0'}, page_content='[Image: page_0_img_0]')],\n",
              " array([[-0.00267245,  0.01282999, -0.05183139, ..., -0.00385086,\n",
              "          0.02977718, -0.00010685],\n",
              "        [ 0.01732335, -0.01327693, -0.02427032, ...,  0.0899405 ,\n",
              "         -0.00272154,  0.03253041]], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create custom FAISS index since we have precomputed embeddings\n",
        "vector_store = FAISS.from_embeddings(\n",
        "    text_embeddings=[(doc.page_content, emb) for doc, emb in zip(all_docs, embeddings_array)],\n",
        "    embedding=None,  # We're using precomputed embeddings\n",
        "    metadatas=[doc.metadata for doc in all_docs]\n",
        ")\n",
        "vector_store"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxRWM_BFX_cf",
        "outputId": "129c9a39-6156-4eb8-f75c-c6d6d5fc8c95"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.faiss.FAISS at 0x7f5b9bd83350>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pE8Ik7r-X_Wp",
        "outputId": "faa7b4f3-80f0-4448-bd0d-9b036a043084"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGoogleGenerativeAI(model='models/gemini-2.5-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x7f5b926af350>, default_metadata=(), model_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_multimodal(query, k=5):\n",
        "    \"\"\"Unified retrieval using CLIP embeddings for both text and images.\"\"\"\n",
        "    # Embed query using CLIP\n",
        "    query_embedding = embed_text(query)\n",
        "\n",
        "    # Search in unified vector store\n",
        "    results = vector_store.similarity_search_by_vector(\n",
        "        embedding=query_embedding,\n",
        "        k=k\n",
        "    )\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "KsiyFjxAZS_P"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_multimodal_message(query, retrieved_docs):\n",
        "    \"\"\"Create a message with both text and images for GPT-4V.\"\"\"\n",
        "    content = []\n",
        "\n",
        "    # Add the query\n",
        "    content.append({\n",
        "        \"type\": \"text\",\n",
        "        \"text\": f\"Question: {query}\\n\\nContext:\\n\"\n",
        "    })\n",
        "\n",
        "    # Separate text and image documents\n",
        "    text_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"text\"]\n",
        "    image_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"image\"]\n",
        "\n",
        "    # Add text context\n",
        "    if text_docs:\n",
        "        text_context = \"\\n\\n\".join([\n",
        "            f\"[Page {doc.metadata['page']}]: {doc.page_content}\"\n",
        "            for doc in text_docs\n",
        "        ])\n",
        "        content.append({\n",
        "            \"type\": \"text\",\n",
        "            \"text\": f\"Text excerpts:\\n{text_context}\\n\"\n",
        "        })\n",
        "\n",
        "    # Add images\n",
        "    for doc in image_docs:\n",
        "        image_id = doc.metadata.get(\"image_id\")\n",
        "        if image_id and image_id in image_data_store:\n",
        "            content.append({\n",
        "                \"type\": \"text\",\n",
        "                \"text\": f\"\\n[Image from page {doc.metadata['page']}]:\\n\"\n",
        "            })\n",
        "            content.append({\n",
        "                \"type\": \"image_url\",\n",
        "                \"image_url\": {\n",
        "                    \"url\": f\"data:image/png;base64,{image_data_store[image_id]}\"\n",
        "                }\n",
        "            })\n",
        "\n",
        "    # Add instruction\n",
        "    content.append({\n",
        "        \"type\": \"text\",\n",
        "        \"text\": \"\\n\\nPlease answer the question based on the provided text and images.\"\n",
        "    })\n",
        "\n",
        "    return HumanMessage(content=content)"
      ],
      "metadata": {
        "id": "eXgH8q6yZhF0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multimodal_pdf_rag_pipeline(query):\n",
        "    \"\"\"Main pipeline for multimodal RAG.\"\"\"\n",
        "    # Retrieve relevant documents\n",
        "    context_docs = retrieve_multimodal(query, k=5)\n",
        "\n",
        "    # Create multimodal message\n",
        "    message = create_multimodal_message(query, context_docs)\n",
        "\n",
        "    # Get response from GPT-4V\n",
        "    response = llm.invoke([message])\n",
        "\n",
        "    # Print retrieved context info\n",
        "    print(f\"\\nRetrieved {len(context_docs)} documents:\")\n",
        "    for doc in context_docs:\n",
        "        doc_type = doc.metadata.get(\"type\", \"unknown\")\n",
        "        page = doc.metadata.get(\"page\", \"?\")\n",
        "        if doc_type == \"text\":\n",
        "            preview = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
        "            print(f\"  - Text from page {page}: {preview}\")\n",
        "        else:\n",
        "            print(f\"  - Image from page {page}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    return response.content"
      ],
      "metadata": {
        "id": "eRbbrIuSaAeG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Example queries\n",
        "    queries = [\n",
        "        \"What does the chart on page 1 show about revenue trends?\",\n",
        "        \"Summarize the main findings from the document\",\n",
        "        \"What visual elements are present in the document?\"\n",
        "    ]\n",
        "\n",
        "    for query in queries:\n",
        "        print(f\"\\nQuery: {query}\")\n",
        "        print(\"-\" * 50)\n",
        "        answer = multimodal_pdf_rag_pipeline(query)\n",
        "        print(f\"Answer: {answer}\")\n",
        "        print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snG2CuAialj8",
        "outputId": "8721199c-86f7-4588-d504-49a6fff0fcdb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: What does the chart on page 1 show about revenue trends?\n",
            "--------------------------------------------------\n",
            "\n",
            "Retrieved 2 documents:\n",
            "  - Text from page 0: Annual Revenue Overview\n",
            "This document summarizes the revenue trends across Q1, Q2, and Q3. As illust...\n",
            "  - Image from page 0\n",
            "\n",
            "\n",
            "Answer: The chart on page 1 (page 0 in the provided context) is a bar chart with three bars of increasing height from left to right.\n",
            "\n",
            "*   The first (blue) bar represents Q1 revenue.\n",
            "*   The second (green) bar represents Q2 revenue and is taller than the first.\n",
            "*   The third (red) bar represents Q3 revenue and is the tallest of all three.\n",
            "\n",
            "Therefore, the chart shows that revenue grew steadily across Q1, Q2, and Q3, with Q3 recording the highest revenue, visually confirming the textual description.\n",
            "======================================================================\n",
            "\n",
            "Query: Summarize the main findings from the document\n",
            "--------------------------------------------------\n",
            "\n",
            "Retrieved 2 documents:\n",
            "  - Text from page 0: Annual Revenue Overview\n",
            "This document summarizes the revenue trends across Q1, Q2, and Q3. As illust...\n",
            "  - Image from page 0\n",
            "\n",
            "\n",
            "Answer: The main findings from the document are:\n",
            "\n",
            "*   **Overall Revenue Trend:** Revenue grew steadily across Q1, Q2, and Q3, with the highest growth recorded in Q3. This is visually supported by the bar chart showing increasing bar heights.\n",
            "*   **Q1 Performance:** Q1 experienced a moderate increase in revenue, driven by the introduction of new product lines.\n",
            "*   **Q2 Performance:** Q2 outperformed Q1, with its growth attributed to marketing campaigns.\n",
            "*   **Q3 Performance:** Q3 showed exponential growth, which was a result of global expansion efforts.\n",
            "======================================================================\n",
            "\n",
            "Query: What visual elements are present in the document?\n",
            "--------------------------------------------------\n",
            "\n",
            "Retrieved 2 documents:\n",
            "  - Text from page 0: Annual Revenue Overview\n",
            "This document summarizes the revenue trends across Q1, Q2, and Q3. As illust...\n",
            "  - Image from page 0\n",
            "\n",
            "\n",
            "Answer: Based on the provided context, the visual elements present in the document are:\n",
            "\n",
            "*   **A bar chart** consisting of three colored bars (blue, green, and red) of increasing height.\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}